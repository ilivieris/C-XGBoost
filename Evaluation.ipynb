{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# Visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "# User's library\n",
    "from utils.PerformanceProfiles import *\n",
    "from utils.non_parametric_tests import *\n",
    "\n",
    "\n",
    "# ACIC, Synthetic\n",
    "Problem = \"ACIC\"\n",
    "\n",
    "# Get files\n",
    "PATH = './Results/'\n",
    "Files = [f for f in listdir(PATH) if isfile(join(PATH, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Methods = []\n",
    "for i, file in enumerate( Files ):\n",
    "    # Select only csv files\n",
    "    #\n",
    "    if ('.csv' not in file): continue\n",
    "        \n",
    "    # Select on results from selected Problem\n",
    "    #\n",
    "    if ( Problem not in file ): continue\n",
    "        \n",
    "        \n",
    "    # Get method name\n",
    "    #\n",
    "    Method = file.split('.csv')[0].split('_')[1]\n",
    "    \n",
    "    Methods += [ Method ]\n",
    "\n",
    "# Sort list with methods\n",
    "Methods.sort()\n",
    "\n",
    "for i, x in enumerate( Methods ):\n",
    "    print('%2i: %s' % (i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Methods/Solvers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectedMethods = [\n",
    "                    \"R-Forest\",\n",
    "                    \"C-Forest\",\n",
    "                    \"DragonNet\",\n",
    "                    \"kNN-DragonNet-euclidean-k=11\",\n",
    "                    \"C-XGBoost\",                    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_ATE  = pd.DataFrame([])\n",
    "Error_PEHE = pd.DataFrame([])\n",
    "\n",
    "for i, file in enumerate( Files ):\n",
    "    # Select only csv files\n",
    "    if ('.csv' not in file): continue\n",
    "        \n",
    "    # Select on results from IHDP\n",
    "    if (Problem != file.split(\"_\")[0]): continue\n",
    "    \n",
    "    \n",
    "    # Select methods\n",
    "    Method = file.split('.csv')[0].split('_')[1]\n",
    "    if (Method not in SelectedMethods): continue\n",
    "    \n",
    "    # Open DataFrame\n",
    "    df = pd.read_csv(PATH + file)\n",
    "    \n",
    "    if (Error_ATE.shape[0] == 0):\n",
    "        Error_ATE['Problem'], Error_PEHE['Problem']  = df.Problem, df.Problem\n",
    "\n",
    "    Error_ATE[Method], Error_PEHE[Method]  = df['Error_ATE'], df['Error_PEHE']\n",
    "\n",
    "    \n",
    "\n",
    "# Set index\n",
    "Error_ATE.set_index('Problem', inplace = True)\n",
    "Error_PEHE.set_index('Problem', inplace = True)\n",
    "\n",
    "# Remove NaN rows if any\n",
    "Error_ATE.dropna(inplace=True)\n",
    "Error_PEHE.dropna(inplace=True)\n",
    "\n",
    "Error_ATE, Error_PEHE = Error_ATE[SelectedMethods], Error_PEHE[SelectedMethods]\n",
    "\n",
    "print('[INFO] nProblem: ', Error_ATE.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61669e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Problem == \"ACIC\":\n",
    "       L = ['ACIC 0', 'ACIC 1', 'ACIC 2', 'ACIC 5', 'ACIC 9', 'ACIC 13', 'ACIC 14',\n",
    "              'ACIC 15', 'ACIC 17', 'ACIC 18', 'ACIC 22', 'ACIC 24', 'ACIC 25',\n",
    "              'ACIC 26', 'ACIC 27', \"ACIC 35\"]\n",
    "\n",
    "       Error_ATE = Error_ATE[ Error_PEHE.index.isin(L) ]\n",
    "       Error_PEHE = Error_PEHE[ Error_PEHE.index.isin(L) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfprof(Error_ATE.values, \n",
    "         linespecs = ['g--', 'm-.', 'b-', 'c-.', 'r-'], \n",
    "         digit = 3,\n",
    "         legendnames = [\"\\\\textsc{R-Forest}\", \"\\\\textsc{C-Forest}\", \"\\\\textsc{Dragonnet}\", \"$k$\\\\textsc{NN-Dragonnet}\", \"\\\\textsc{C-XGBoost}\", ],\n",
    "        #  legendnames = Error_ATE.columns,\n",
    "         thmax = 30, figsize = (8, 5) )\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.savefig( 'images/{}_Error_ATE.png'.format(Problem), dpi = 300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error PEHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] nProblem: ', Error_ATE.shape[0])\n",
    "\n",
    "perfprof(Error_PEHE,\n",
    "         linespecs = ['g--', 'm-.', 'b-', 'c-.', 'r-'], \n",
    "         digit = 4,\n",
    "         legendnames = [\"\\\\textsc{R-Forest}\", \"\\\\textsc{C-Forest}\", \"\\\\textsc{Dragonnet}\", \"$k$\\\\textsc{NN-Dragonnet}\", \"\\\\textsc{C-XGBoost}\", ],\n",
    "        #  legendnames = Error_ATE.columns,\n",
    "         thmax = 20, figsize = (8, 5) )\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.savefig( 'images/{}_Error_PEHE.png'.format(Problem), dpi = 300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friedman Aligned Ranking (FAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, p_value, rankings_avg, rankings_cmp = friedman_aligned_ranks_test( Error_ATE )\n",
    "\n",
    "\n",
    "# Summary\n",
    "#\n",
    "print('\\n')\n",
    "print('[INFO] H0: {All methods exhibited similar results with no statistical differences}')\n",
    "print('[INFO] FAR: %.3f (p-value: %.5f)' % (T, p_value))\n",
    "if (p_value < 0.05):\n",
    "    print('\\t> H0 is rejected')\n",
    "else:\n",
    "    print('\\t> H0 is failed to be rejected')\n",
    "\n",
    "\n",
    "Ranking            = pd.DataFrame( [] )\n",
    "Ranking['Methods'] = Error_ATE.columns\n",
    "Ranking['FAR']     = rankings_avg\n",
    "\n",
    "Ranking = Ranking.sort_values(by           = 'FAR', \n",
    "                              ignore_index = True)\n",
    "Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finner post-hoc test\n",
    "\n",
    "**Multiple comparisons ($1 \\times N$)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with rankings\n",
    "#\n",
    "d = {}\n",
    "for i, feature in enumerate( Error_ATE.columns ):\n",
    "    d[ feature ] = rankings_cmp[i] \n",
    "    \n",
    "comparisons, z_values, p_values, adj_p_values = finner_test( d )\n",
    "\n",
    "Finner = pd.DataFrame( [] )\n",
    "Finner['Comparisons']     = comparisons\n",
    "Finner['APV']             = adj_p_values\n",
    "Finner['Null hypothesis'] = Finner['APV'].apply(lambda x: 'Rejected' if x < 0.05 else 'Failed to reject')\n",
    "\n",
    "Finner = Finner.sort_values(by = 'APV', ascending = False, ignore_index = True)\n",
    "Finner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friedman Aligned Ranking (FAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, p_value, rankings_avg, rankings_cmp = friedman_aligned_ranks_test( Error_PEHE )\n",
    "\n",
    "\n",
    "# Summary\n",
    "#\n",
    "print('\\n')\n",
    "print('[INFO] H0: {All methods exhibited similar results with no statistical differences}')\n",
    "print('[INFO] FAR: %.3f (p-value: %.5f)' % (T, p_value))\n",
    "if (p_value < 0.05):\n",
    "    print('\\t> H0 is rejected')\n",
    "else:\n",
    "    print('\\t> H0 is failed to be rejected')\n",
    "\n",
    "\n",
    "Ranking            = pd.DataFrame( [] )\n",
    "Ranking['Methods'] = Error_PEHE.columns\n",
    "Ranking['FAR']     = rankings_avg\n",
    "\n",
    "Ranking = Ranking.sort_values(by           = 'FAR', \n",
    "                              ignore_index = True)\n",
    "Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efeba84",
   "metadata": {},
   "source": [
    "#### Finner post-hoc test\n",
    "\n",
    "**Multiple comparisons ($1 \\times N$)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with rankings\n",
    "#\n",
    "d = {}\n",
    "for i, feature in enumerate( Error_PEHE.columns ):\n",
    "    d[ feature ] = rankings_cmp[i] \n",
    "    \n",
    "comparisons, z_values, p_values, adj_p_values = finner_test( d )\n",
    "\n",
    "Finner = pd.DataFrame( [] )\n",
    "Finner['Comparisons']     = comparisons\n",
    "Finner['APV']             = adj_p_values\n",
    "Finner['Null hypothesis'] = Finner['APV'].apply(lambda x: 'Rejected' if x < 0.05 else 'Failed to reject')\n",
    "\n",
    "Finner = Finner.sort_values(by = 'APV', ascending = False, ignore_index = True)\n",
    "Finner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
