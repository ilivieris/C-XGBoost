{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] All libraries were imported\n",
      "[INFO] Random generators were initialized\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Basic libraries\n",
    "import random\n",
    "import time\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# Sklearn library\n",
    "from sklearn.preprocessing   import StandardScaler, RobustScaler\n",
    "# User libraries\n",
    "from utils.data_loading import Synthetic_dataset, TWINS_dataset, IHDP_dataset, ACIC_dataset\n",
    "from utils.metrics      import PEHE, ATE\n",
    "print('[INFO] All libraries were imported')\n",
    "\n",
    "\n",
    "\n",
    "# Random generators initialization\n",
    "seed=42\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "print('[INFO] Random generators were initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"TWINS\" # {\"IHDP\", \"Synthetic\", \"TWINS\", \"ACIC\"}\n",
    "path = \"Data/TWINS/\" # {\"Data/Synthetic/\", \"Data/IHDP/\", \"Data/TWINS/\", \"Data/ACIC/\"}\n",
    "filename = f\"./Results/{problem}_C-XGBoost.csv\"\n",
    "\n",
    "\n",
    "if \"Synthetic\" in problem:\n",
    "    DataLoader = Synthetic_dataset(path=path)\n",
    "elif \"IHDP\" in problem:\n",
    "    DataLoader = IHDP_dataset(path=path)\n",
    "elif \"TWINS\" in problem:\n",
    "    DataLoader = TWINS_dataset(path=path)\n",
    "elif \"ACIC\" in problem:\n",
    "    DataLoader = ACIC_dataset(path=path, train_size=0.8, random_state=1983)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation:  0\n",
      "[INFO] Dataset imported\n",
      "[INFO] Number of training instances:  19124\n",
      "[0]\tvalidation_0-rmse:0.50000\tvalidation_1-rmse:0.50000\n",
      "[50]\tvalidation_0-rmse:0.50000\tvalidation_1-rmse:0.50000\n",
      "[99]\tvalidation_0-rmse:0.50000\tvalidation_1-rmse:0.50000\n",
      "[INFO] Model trained\n",
      "[INFO] Time 5.74 seconds\n",
      "\n",
      "\n",
      "Simulation:  1\n",
      "[INFO] Dataset imported\n",
      "[INFO] Number of training instances:  19124\n",
      "[0]\tvalidation_0-rmse:0.50000\tvalidation_1-rmse:0.50000\n",
      "[50]\tvalidation_0-rmse:0.50000\tvalidation_1-rmse:0.50000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ioann\\Repositories\\Project under development\\C-XGBoost\\C-XGBoost.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     yt_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([trainY\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), trainT\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# model.fit(trainX, yt_train);\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(trainX, yt_train, eval_set\u001b[39m=\u001b[39;49m[(trainX, train_potential_Y), (testX, test_potential_Y)], verbose\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m);\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[INFO] Model trained\u001b[39m\u001b[39m'\u001b[39m)    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m# *** Predictions ***\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# Get predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ioann/Repositories/Project%20under%20development/C-XGBoost/C-XGBoost.ipynb#W2sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39m#\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Repositories\\Project under development\\C-XGBoost\\env1\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Repositories\\Project under development\\C-XGBoost\\env1\\lib\\site-packages\\xgboost\\sklearn.py:1086\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m (\n\u001b[0;32m   1078\u001b[0m     model,\n\u001b[0;32m   1079\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1084\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1085\u001b[0m )\n\u001b[1;32m-> 1086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1087\u001b[0m     params,\n\u001b[0;32m   1088\u001b[0m     train_dmatrix,\n\u001b[0;32m   1089\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1090\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1091\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1092\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1093\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1094\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1095\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1096\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1097\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1098\u001b[0m )\n\u001b[0;32m   1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Repositories\\Project under development\\C-XGBoost\\env1\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Repositories\\Project under development\\C-XGBoost\\env1\\lib\\site-packages\\xgboost\\training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     bst\u001b[39m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[1;32m--> 182\u001b[0m     \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39;49mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    185\u001b[0m bst \u001b[39m=\u001b[39m cb_container\u001b[39m.\u001b[39mafter_training(bst)\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Repositories\\Project under development\\C-XGBoost\\env1\\lib\\site-packages\\xgboost\\callback.py:238\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[1;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mfor\u001b[39;00m _, name \u001b[39min\u001b[39;00m evals:\n\u001b[0;32m    237\u001b[0m     \u001b[39massert\u001b[39;00m name\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDataset name should not contain `-`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 238\u001b[0m score: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49meval_set(evals, epoch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetric, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_margin)\n\u001b[0;32m    239\u001b[0m metric_score \u001b[39m=\u001b[39m _parse_eval_str(score)\n\u001b[0;32m    240\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Repositories\\Project under development\\C-XGBoost\\env1\\lib\\site-packages\\xgboost\\core.py:2125\u001b[0m, in \u001b[0;36mBooster.eval_set\u001b[1;34m(self, evals, iteration, feval, output_margin)\u001b[0m\n\u001b[0;32m   2122\u001b[0m evnames \u001b[39m=\u001b[39m c_array(ctypes\u001b[39m.\u001b[39mc_char_p, [c_str(d[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m evals])\n\u001b[0;32m   2123\u001b[0m msg \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_char_p()\n\u001b[0;32m   2124\u001b[0m _check_call(\n\u001b[1;32m-> 2125\u001b[0m     _LIB\u001b[39m.\u001b[39;49mXGBoosterEvalOneIter(\n\u001b[0;32m   2126\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   2127\u001b[0m         ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   2128\u001b[0m         dmats,\n\u001b[0;32m   2129\u001b[0m         evnames,\n\u001b[0;32m   2130\u001b[0m         c_bst_ulong(\u001b[39mlen\u001b[39;49m(evals)),\n\u001b[0;32m   2131\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(msg),\n\u001b[0;32m   2132\u001b[0m     )\n\u001b[0;32m   2133\u001b[0m )\n\u001b[0;32m   2134\u001b[0m \u001b[39massert\u001b[39;00m msg\u001b[39m.\u001b[39mvalue \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2135\u001b[0m res \u001b[39m=\u001b[39m msg\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mdecode()  \u001b[39m# pylint: disable=no-member\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {'ATE': [], 'Error_ATE': [], 'Error_PEHE':[]} \n",
    "for idx in range(DataLoader.nProblems):\n",
    "\n",
    "    # Start timer\n",
    "    #\n",
    "    start1 = time.time()\n",
    "    \n",
    "    \n",
    "    # Load training data\n",
    "    trainX, trainT, trainY, train_potential_Y = DataLoader.getTraining( idx )\n",
    "\n",
    "    # Load testing data\n",
    "    testX, testT, testY, test_potential_Y  = DataLoader.getTesting( idx )\n",
    "    #\n",
    "    print('Simulation: ', idx)\n",
    "    print('[INFO] Dataset imported')\n",
    "    print('[INFO] Number of training instances: ', trainX.shape[0])\n",
    "    \n",
    " \n",
    "    # Setup scaler for inputs\n",
    "    #\n",
    "    scalerX = RobustScaler()\n",
    "    trainX  = scalerX.fit_transform( trainX )\n",
    "    testX   = scalerX.transform( testX )\n",
    "\n",
    "\n",
    "    tt = np.array([[1,0] if x == 0 else [0,1] for x in trainT]).flatten()\n",
    "\n",
    "    def custom_loss(y_true:np.ndarray=None, y_pred:np.ndarray=None)->(np.ndarray,np.ndarray):\n",
    "        grad = 2*(y_pred.flatten() - y_true.flatten()) * tt\n",
    "        hess = (0*y_pred.flatten() + 2)  * tt\n",
    "\n",
    "        return grad, hess\n",
    "    \n",
    "\n",
    "\n",
    "    # Setup XGBoost\n",
    "    #\n",
    "    # import xgboost\n",
    "    # model = xgboost.XGBRegressor(n_estimators=200, \n",
    "    #                              max_depth=4, \n",
    "    #                              objective=custom_loss, \n",
    "    #                              learning_rate=1e-2, \n",
    "    #                             #  booster=\"gblinear\",\n",
    "    #                              n_jobs=-1,\n",
    "    #                              tree_method=\"hist\", \n",
    "    #                              multi_strategy=\"multi_output_tree\")\n",
    "    import xgboost\n",
    "    model = xgboost.XGBRegressor(n_estimators=200, \n",
    "                                 max_depth=5, \n",
    "                                 objective=custom_loss, \n",
    "                                 learning_rate=5e-5, \n",
    "                                #  booster=\"gblinear\",\n",
    "                                 #\n",
    "                                 reg_alpha = 0,\n",
    "                                 reg_lambda = 1.0,\n",
    "                                 gamma = 1.0,\n",
    "                                 min_child_weight = 2.0,\n",
    "                                 max_leaves = 2,\n",
    "                                 #\n",
    "                                 n_jobs=-1,\n",
    "                                 tree_method=\"hist\", \n",
    "                                 multi_strategy=\"multi_output_tree\")\n",
    "    \n",
    "    # Create outputs for DragonNet (concatenate Y & T)\n",
    "    #\n",
    "    if \"ACIC\" in problem:\n",
    "        scalerY       = RobustScaler()\n",
    "        trainY_scaled = scalerY.fit_transform( trainY.reshape(-1,1) ).squeeze(-1)      \n",
    "        yt_train = np.concatenate([trainY_scaled.reshape(-1,1), trainT.reshape(-1,1)], axis = 1)\n",
    "    else:\n",
    "        yt_train = np.concatenate([trainY.reshape(-1,1), trainT.reshape(-1,1)], axis = 1)\n",
    "    \n",
    "\n",
    "    # Train model\n",
    "    #\n",
    "    # model.fit(trainX, yt_train);\n",
    "    model.fit(trainX, yt_train, eval_set=[(trainX, train_potential_Y), (testX, test_potential_Y)], verbose=50);\n",
    "    print('[INFO] Model trained')    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #\n",
    "    #\n",
    "    # *** Predictions ***\n",
    "    #\n",
    "    #       \n",
    "\n",
    "    # Get predictions\n",
    "    #\n",
    "    test_y_hat = model.predict( testX )\n",
    "\n",
    "    if \"ACIC\" in problem:\n",
    "        test_y_hat[:,0] = scalerY.inverse_transform( test_y_hat[:,0].reshape(-1,1) ).squeeze()\n",
    "        test_y_hat[:,1] = scalerY.inverse_transform( test_y_hat[:,1].reshape(-1,1) ).squeeze()    \n",
    "    \n",
    "    # ATE\n",
    "    #\n",
    "    real_ATE = ( test_potential_Y[:,1] - test_potential_Y[:,0] ).mean()\n",
    "    \n",
    "    \n",
    "    # Error PEHE\n",
    "    #\n",
    "    Error_PEHE = PEHE(test_potential_Y, test_y_hat)\n",
    "    \n",
    "    \n",
    "    # Error ATE\n",
    "    #\n",
    "    Error_ATE = ATE(test_potential_Y, test_y_hat)  \n",
    "    \n",
    "        \n",
    "    # Store errors of PEHE and ATE\n",
    "    #\n",
    "    results['ATE']            += [ np.round(real_ATE,   6) ]\n",
    "    results['Error_ATE']      += [ np.round(Error_ATE,  6) ]\n",
    "    results['Error_PEHE']     += [ np.round(Error_PEHE, 6) ]\n",
    "    print('[INFO] Time %.2f seconds\\n\\n' % (time.time() - start1))\n",
    "\n",
    "    # from sklearn import metrics\n",
    "    # print('0: ', metrics.mean_absolute_error(test_potential_Y[:,0], test_y_hat[:,0]))\n",
    "    # print('1: ', metrics.mean_absolute_error(test_potential_Y[:,1], test_y_hat[:,1]))    \n",
    "\n",
    "\n",
    "    \n",
    "    # Save results (at each iteration)\n",
    "    df = pd.DataFrame( results )\n",
    "    df['Problem'] = [f\"{problem} {x}\" for x in df.index]\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    # print(results['Error_ATE'])\n",
    "    # print(results['Error_PEHE'])\n",
    "    # print(50*\"-\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2157, 16967)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(trainT==0)[0].size, np.where(trainT==1)[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
